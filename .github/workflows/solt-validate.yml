# =============================================================================
# SOLT PRE-COMMIT - Reusable Workflow
# =============================================================================
# IMPORTANT: This workflow will FAIL (red) when blocking issues are found.
# The validation job exit code is properly propagated via the final-check step.
# =============================================================================
name: Solt Validation (Reusable)

on:
  workflow_call:
    inputs:
      python-version:
        description: 'Python version to use'
        required: false
        type: string
        default: '3.11'
      odoo-version:
        description: 'Odoo version (for compatibility checks)'
        required: false
        type: string
        default: '17.0'
      validation-scope:
        description: 'Validation scope: changed or full'
        required: false
        type: string
        default: 'changed'
      fail-on-warnings:
        description: 'Fail if warnings are found'
        required: false
        type: boolean
        default: false
      show-info:
        description: 'Show info-level issues in report'
        required: false
        type: boolean
        default: false
      gist-id:
        description: 'Gist ID for badge storage (optional)'
        required: false
        type: string
        default: ''
      badge-filename-prefix:
        description: 'Prefix for badge filenames in gist (e.g., solt-budget)'
        required: false
        type: string
        default: ''
      docstring-threshold:
        description: 'Minimum docstring coverage to pass (default: 80)'
        required: false
        type: number
        default: 80
    secrets:
      GIST_SECRET:
        description: 'GitHub token for updating gist badges'
        required: false
    outputs:
      validation-result:
        description: 'Validation result (success/failure)'
        value: ${{ jobs.validate.outputs.result }}
      errors-count:
        description: 'Number of errors found'
        value: ${{ jobs.validate.outputs.errors }}
      warnings-count:
        description: 'Number of warnings found'
        value: ${{ jobs.validate.outputs.warnings }}
      docstring-coverage:
        description: 'Docstring coverage percentage'
        value: ${{ jobs.validate.outputs.docstring_cov }}

jobs:
  # ---------------------------------------------------------------------------
  # BRANCH VALIDATION
  # ---------------------------------------------------------------------------
  branch-check:
    name: Branch Name
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}
      - name: Install solt-pre-commit
        run: pip install "git+https://github.com/soltein-net/solt-pre-commit.git@v1.0.0"
      - name: Validate branch name
        run: |
          BRANCH="${{ github.head_ref }}"
          echo "## Branch Validation" >> $GITHUB_STEP_SUMMARY
          echo "Branch: \`$BRANCH\`" >> $GITHUB_STEP_SUMMARY
          if solt-check-branch "$BRANCH"; then
            echo "Branch name is valid" >> $GITHUB_STEP_SUMMARY
          else
            echo "Branch name is invalid" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  # ---------------------------------------------------------------------------
  # MAIN VALIDATION
  # ---------------------------------------------------------------------------
  validate:
    name: Odoo Module Validation
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    outputs:
      result: ${{ steps.final-check.outputs.result }}
      errors: ${{ steps.summary.outputs.errors }}
      warnings: ${{ steps.summary.outputs.warnings }}
      docstring_cov: ${{ steps.metrics.outputs.docstring_cov }}
      docs_status: ${{ steps.metrics.outputs.docs_status }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install "git+https://github.com/soltein-net/solt-pre-commit.git@v1.0.0"
          pip install pylint-odoo ruff

      - name: Find Odoo modules
        id: find-modules
        run: |
          MODULES=$(find . -name "__manifest__.py" -o -name "__openerp__.py" | xargs -I {} dirname {} | sort -u | tr '\n' ' ')
          MODULE_COUNT=$(echo "$MODULES" | wc -w | tr -d ' ')
          if [ -z "$MODULE_COUNT" ] || [ "$MODULE_COUNT" = "" ]; then
            MODULE_COUNT=0
          fi
          echo "modules=$MODULES" >> $GITHUB_OUTPUT
          echo "count=$MODULE_COUNT" >> $GITHUB_OUTPUT
          echo "Found $MODULE_COUNT modules: $MODULES"

      # NOTE: This step uses continue-on-error so subsequent steps run.
      # The actual pass/fail is determined by "Final validation check" at the end.
      - name: Run validation (collect results)
        id: validate
        continue-on-error: true
        run: |
          mkdir -p reports
          ARGS="--scope ${{ inputs.validation-scope }}"
          if [ "${{ inputs.show-info }}" = "true" ]; then
            ARGS="$ARGS --show-info"
          fi

          START_TIME=$(date +%s)

          # Run validation and capture exit code
          set -o pipefail
          set +e
          solt-check-odoo ${{ steps.find-modules.outputs.modules }} $ARGS 2>&1 | tee reports/validation.txt
          VALIDATION_EXIT_CODE=${PIPESTATUS[0]}
          set -e
          set +o pipefail

          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))

          echo "exit_code=$VALIDATION_EXIT_CODE" >> $GITHUB_OUTPUT
          echo "duration=$DURATION" >> $GITHUB_OUTPUT

          # Show exit code in logs for debugging
          echo "::notice::Validation exit code: $VALIDATION_EXIT_CODE"
          
          # Exit with validation code to store in step outcome
          exit $VALIDATION_EXIT_CODE

      - name: Run full validation for metrics
        if: always()
        continue-on-error: true
        run: |
          solt-check-odoo ${{ steps.find-modules.outputs.modules }} --scope full --show-info 2>&1 | tee reports/validation-full.txt || true

      - name: Run Ruff check
        id: ruff
        if: always()
        continue-on-error: true
        run: |
          set +e
          
          echo "=== Modules to check: ${{ steps.find-modules.outputs.modules }} ==="
          
          # Get changed Python files
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            git fetch origin ${{ github.base_ref }} --depth=1 2>/dev/null || true
            CHANGED_PY=$(git diff --name-only --diff-filter=ACMR origin/${{ github.base_ref }}...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          else
            CHANGED_PY=$(git diff --name-only --diff-filter=ACMR HEAD~1...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          fi

          RUFF_CHANGED="0"
          if [ -n "$CHANGED_PY" ]; then
            EXISTING_PY=""
            for f in $CHANGED_PY; do
              if [ -f "$f" ]; then
                EXISTING_PY="$EXISTING_PY $f"
              fi
            done
            EXISTING_PY=$(echo "$EXISTING_PY" | xargs)
            
            if [ -n "$EXISTING_PY" ]; then
              echo "=== Ruff checking changed files ==="
              ruff check $EXISTING_PY --output-format=concise 2>&1 | tee reports/ruff-changed.txt || true
              RUFF_CHANGED=$(grep -cE "^.+\.py:[0-9]+" reports/ruff-changed.txt 2>/dev/null || echo "0")
            fi
          fi

          # Run Ruff on full repo
          echo "=== Running Ruff on full repository ==="
          ALL_PY_FILES=$(find ${{ steps.find-modules.outputs.modules }} -name "*.py" -not -path "*/__pycache__/*" 2>/dev/null | tr '\n' ' ')
          PY_FILE_COUNT=$(echo "$ALL_PY_FILES" | wc -w | tr -d ' ')
          echo "Found $PY_FILE_COUNT Python files to check"
          
          RUFF_FULL="0"
          if [ -n "$ALL_PY_FILES" ] && [ "$PY_FILE_COUNT" -gt 0 ]; then
            ruff check $ALL_PY_FILES --output-format=concise 2>&1 | tee reports/ruff-full.txt || true
            RUFF_FULL=$(grep -cE "^.+\.py:[0-9]+" reports/ruff-full.txt 2>/dev/null || echo "0")
          fi
          
          # Debug output
          echo "=== Ruff full report (first 20 lines): ==="
          head -20 reports/ruff-full.txt 2>/dev/null || echo "(empty)"

          # CRITICAL: Sanitize values - remove ALL non-numeric characters
          RUFF_CHANGED=$(printf '%s' "$RUFF_CHANGED" | tr -cd '0-9')
          RUFF_FULL=$(printf '%s' "$RUFF_FULL" | tr -cd '0-9')
          
          # Ensure non-empty
          : "${RUFF_CHANGED:=0}"
          : "${RUFF_FULL:=0}"
          
          echo "=== FINAL COUNTS ==="
          echo "RUFF_CHANGED: $RUFF_CHANGED"
          echo "RUFF_FULL: $RUFF_FULL"
          echo "===================="

          # Write to GITHUB_OUTPUT using printf to avoid newline issues
          printf 'changed=%s\n' "$RUFF_CHANGED" >> "$GITHUB_OUTPUT"
          printf 'full=%s\n' "$RUFF_FULL" >> "$GITHUB_OUTPUT"

      - name: Run Pylint-Odoo check
        id: pylint
        if: always()
        continue-on-error: true
        run: |
          set +e
          # Get changed Python files, EXCLUDING deleted files (--diff-filter=ACMR)
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            CHANGED_PY=$(git diff --name-only --diff-filter=ACMR origin/${{ github.base_ref }}...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          else
            CHANGED_PY=$(git diff --name-only --diff-filter=ACMR HEAD~1...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          fi

          PYLINT_CHANGED=0
          if [ -n "$CHANGED_PY" ]; then
            # Filter out files that don't exist (extra safety)
            EXISTING_PY=""
            for f in $CHANGED_PY; do
              if [ -f "$f" ]; then
                EXISTING_PY="$EXISTING_PY $f"
              fi
            done
            EXISTING_PY=$(echo "$EXISTING_PY" | xargs)
            
            if [ -n "$EXISTING_PY" ]; then
              echo "$EXISTING_PY" | xargs pylint --load-plugins=pylint_odoo --exit-zero 2>&1 | tee reports/pylint-changed.txt || true
              PYLINT_CHANGED=$(grep -cE "^[CRWEF][0-9]{4}:" reports/pylint-changed.txt 2>/dev/null || echo 0)
            fi
          fi

          find ${{ steps.find-modules.outputs.modules }} -name "*.py" -not -path "*/__pycache__/*" 2>/dev/null | head -100 | xargs pylint --load-plugins=pylint_odoo --exit-zero 2>&1 | tee reports/pylint-full.txt || true
          PYLINT_FULL=$(grep -cE "^[CRWEF][0-9]{4}:" reports/pylint-full.txt 2>/dev/null || echo 0)

          PYLINT_CHANGED=$(echo "$PYLINT_CHANGED" | grep -oE '^[0-9]+' || echo 0)
          PYLINT_FULL=$(echo "$PYLINT_FULL" | grep -oE '^[0-9]+' || echo 0)

          echo "changed=${PYLINT_CHANGED:-0}" >> $GITHUB_OUTPUT
          echo "full=${PYLINT_FULL:-0}" >> $GITHUB_OUTPUT

      - name: Calculate real metrics
        id: metrics
        if: always()
        env:
          DOCSTRING_THRESHOLD: ${{ inputs.docstring-threshold }}
        run: |
          # Debug: Show if validation files exist
          echo "=== Debug: Checking report files ==="
          ls -la reports/ 2>/dev/null || echo "reports/ directory not found"
          
          if [ -f "reports/validation-full.txt" ]; then
            echo "=== Last 30 lines of validation-full.txt ==="
            tail -30 reports/validation-full.txt
            echo "=== Searching for METRICS: line ==="
            grep "METRICS:" reports/validation-full.txt || echo "METRICS: line not found"
          else
            echo "validation-full.txt not found"
          fi
          
          python3 << 'EOF'
          import os
          import re
          import json

          print("=== Python metrics script starting ===")

          metrics_data = {
              'docstring_cov': 0, 'docstring_documented': 0, 'docstring_total': 1,
              'string_cov': 0, 'string_documented': 0, 'string_total': 1,
              'help_cov': 0, 'help_documented': 0, 'help_total': 1,
              'models': 0
          }

          try:
              with open('reports/validation-full.txt', 'r') as f:
                  content = f.read()
              print(f"Read {len(content)} bytes from validation-full.txt")
          
              metrics_match = re.search(r'^METRICS:(.+)$', content, re.MULTILINE)
              if metrics_match:
                  print(f"Found METRICS line: {metrics_match.group(1)[:100]}...")
                  for pair in metrics_match.group(1).split(','):
                      if '=' in pair:
                          key, value = pair.split('=', 1)
                          try:
                              metrics_data[key.strip()] = float(value) if '.' in value else int(value)
                          except ValueError:
                              print(f"  Failed to parse: {pair}")
              else:
                  print("METRICS: line NOT found in file")
                  # Try to find any line containing METRICS
                  for line in content.split('\n'):
                      if 'METRICS' in line or 'metrics' in line.lower():
                          print(f"  Found similar line: {line[:100]}")
          except FileNotFoundError:
              print("ERROR: validation-full.txt not found")
          except Exception as e:
              print(f"ERROR reading validation-full.txt: {e}")

          missing_docstring_pr = missing_string_pr = missing_help_pr = 0
          try:
              with open('reports/validation.txt', 'r') as f:
                  pr_content = f.read().lower()
              missing_docstring_pr = pr_content.count('missing docstring')
              missing_string_pr = pr_content.count('missing string')
              missing_help_pr = pr_content.count('missing help')
          except FileNotFoundError:
              pass

          docstring_cov = metrics_data.get('docstring_cov', 0)
          threshold = int(os.environ.get('DOCSTRING_THRESHOLD', 80))
          docs_status = 'passing' if docstring_cov >= threshold else 'failing'

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"docstring_cov={int(docstring_cov)}\n")
              f.write(f"docstring_documented={int(metrics_data.get('docstring_documented', 0))}\n")
              f.write(f"docstring_total={int(metrics_data.get('docstring_total', 1))}\n")
              f.write(f"string_cov={int(metrics_data.get('string_cov', 0))}\n")
              f.write(f"string_documented={int(metrics_data.get('string_documented', 0))}\n")
              f.write(f"string_total={int(metrics_data.get('string_total', 1))}\n")
              f.write(f"help_cov={int(metrics_data.get('help_cov', 0))}\n")
              f.write(f"help_documented={int(metrics_data.get('help_documented', 0))}\n")
              f.write(f"help_total={int(metrics_data.get('help_total', 1))}\n")
              f.write(f"models={int(metrics_data.get('models', 0))}\n")
              f.write(f"missing_docstring_pr={missing_docstring_pr}\n")
              f.write(f"missing_string_pr={missing_string_pr}\n")
              f.write(f"missing_help_pr={missing_help_pr}\n")
              f.write(f"docs_status={docs_status}\n")

          with open('reports/metrics.json', 'w') as f:
              json.dump(metrics_data, f, indent=2)

          print(f"=== Final metrics ===")
          print(f"Docstrings: {docstring_cov:.1f}% ({metrics_data.get('docstring_documented', 0)}/{metrics_data.get('docstring_total', 1)})")
          print(f"Field strings: {metrics_data.get('string_cov', 0):.1f}%")
          print(f"Field help: {metrics_data.get('help_cov', 0):.1f}%")
          print(f"Status: {docs_status}")
          EOF

          echo "ruff_changed=${{ steps.ruff.outputs.changed }}" >> $GITHUB_OUTPUT
          echo "ruff_full=${{ steps.ruff.outputs.full }}" >> $GITHUB_OUTPUT
          echo "pylint_changed=${{ steps.pylint.outputs.changed }}" >> $GITHUB_OUTPUT
          echo "pylint_full=${{ steps.pylint.outputs.full }}" >> $GITHUB_OUTPUT

      - name: Generate summary and extract errors
        id: summary
        if: always()
        run: |
          # Create reports directory if it doesn't exist
          mkdir -p reports
          touch reports/validation.txt
          
          EXIT_CODE="${{ steps.validate.outputs.exit_code }}"
          EXIT_CODE=$(echo "$EXIT_CODE" | grep -oE '^[0-9]+' | head -1 || echo "0")
          if [ -z "$EXIT_CODE" ]; then EXIT_CODE="0"; fi

          # Count errors/warnings from FINAL SUMMARY section
          # Format: "  Errors: 0" and "  Warnings: 48"
          ERRORS="0"
          WARNINGS="0"
          if [ -f "reports/validation.txt" ]; then
            # Try to extract from summary lines like "  Errors: 0"
            ERRORS=$(grep -oP "Errors:\s*\K\d+" reports/validation.txt 2>/dev/null | tail -1 || echo "0")
            WARNINGS=$(grep -oP "Warnings:\s*\K\d+" reports/validation.txt 2>/dev/null | tail -1 || echo "0")
          fi
          # Ensure they are single numbers
          ERRORS=$(echo "$ERRORS" | grep -oE '^[0-9]+' | head -1 || echo "0")
          WARNINGS=$(echo "$WARNINGS" | grep -oE '^[0-9]+' | head -1 || echo "0")
          if [ -z "$ERRORS" ]; then ERRORS="0"; fi
          if [ -z "$WARNINGS" ]; then WARNINGS="0"; fi
          
          # Write outputs safely
          {
            echo "errors=$ERRORS"
            echo "warnings=$WARNINGS"
            echo "validation_exit_code=$EXIT_CODE"
          } >> "$GITHUB_OUTPUT"
          
          echo "Summary: exit_code=$EXIT_CODE, errors=$ERRORS, warnings=$WARNINGS"

          # Extract first 10 errors/warnings for PR comment (full list is in workflow logs)
          python3 << 'PYEOF'
          import os
          import re
          import json

          # Maximum number of issues to extract for PR comment
          # Full list is shown in workflow logs via solt-check-odoo output
          MAX_ERRORS = 10
          MAX_WARNINGS = 10

          errors_list = []
          warnings_list = []
          total_errors = 0
          total_warnings = 0

          try:
              with open('reports/validation.txt', 'r', encoding='utf-8', errors='replace') as f:
                  content = f.read()

              # First, try to extract totals from FINAL SUMMARY section
              errors_match = re.search(r'Errors:\s*(\d+)', content)
              warnings_match = re.search(r'Warnings:\s*(\d+)', content)
              if errors_match:
                  total_errors = int(errors_match.group(1))
              if warnings_match:
                  total_warnings = int(warnings_match.group(1))

              current_severity = None
              current_check = None

              for line in content.split('\n'):
                  line_stripped = line.strip()

                  # Detect severity section headers (with emoji or bracket format)
                  if 'ERRORS' in line_stripped.upper() and '(' in line_stripped:
                      current_severity = 'error'
                      match = re.search(r'\((\d+)\)', line_stripped)
                      if match and total_errors == 0:
                          total_errors = int(match.group(1))
                      continue
                  elif 'WARNING' in line_stripped.upper() and '(' in line_stripped:
                      current_severity = 'warning'
                      match = re.search(r'\((\d+)\)', line_stripped)
                      if match and total_warnings == 0:
                          total_warnings = int(match.group(1))
                      continue
                  elif 'INFO' in line_stripped.upper() and '(' in line_stripped:
                      current_severity = 'info'
                      continue

                  # Skip separators and empty lines
                  if not line_stripped or line_stripped.startswith('---') or line_stripped.startswith('==='):
                      continue

                  # Detect check name headers like "  Python Field Missing String (2)"
                  check_header_match = re.match(r'^  ([A-Z][^(]+)\s*\((\d+)\)', line)
                  if check_header_match:
                      current_check = check_header_match.group(1).strip()
                      continue

                  # Parse error/warning lines starting with "    - "
                  if line.startswith('    - ') and current_severity in ('error', 'warning'):
                      msg_content = line[6:]  # Remove "    - "

                      # Try to extract file:line pattern
                      file_line_match = re.match(r'^([^\s:]+):(\d+)\s+(.+)$', msg_content)

                      if file_line_match:
                          filepath = file_line_match.group(1)
                          lineno = file_line_match.group(2)
                          message = file_line_match.group(3)

                          # Shorten filepath - keep last 2 parts
                          parts = filepath.replace('\\', '/').split('/')
                          if len(parts) > 2:
                              filepath = '/'.join(parts[-2:])

                          # Truncate long messages
                          if len(message) > 80:
                              message = message[:77] + '...'

                          entry = {
                              'file': filepath,
                              'line': lineno,
                              'message': message,
                              'check': current_check
                          }
                      else:
                          # No file:line pattern, just message
                          message = msg_content
                          if len(message) > 100:
                              message = message[:97] + '...'
                          entry = {
                              'file': '',
                              'line': '',
                              'message': message,
                              'check': current_check
                          }

                      if current_severity == 'error' and len(errors_list) < MAX_ERRORS:
                          errors_list.append(entry)
                      elif current_severity == 'warning' and len(warnings_list) < MAX_WARNINGS:
                          warnings_list.append(entry)

          except Exception as e:
              print(f"Error parsing validation output: {e}")
              import traceback
              traceback.print_exc()

          # Save to JSON for the comment step
          result = {
              'errors': errors_list,
              'warnings': warnings_list,
              'total_errors': total_errors,
              'total_warnings': total_warnings
          }

          with open('reports/extracted_issues.json', 'w') as f:
              json.dump(result, f, indent=2)

          print(f"Extracted {len(errors_list)}/{total_errors} errors, {len(warnings_list)}/{total_warnings} warnings")

          # Debug: show what we extracted
          if errors_list:
              print("Sample errors:")
              for e in errors_list[:5]:
                  print(f"  - {e.get('file')}:{e.get('line')} {e.get('message')}")
          
          # Also parse Pylint errors if they exist
          pylint_errors = []
          try:
              for pylint_file in ['reports/pylint-changed.txt', 'reports/pylint-full.txt']:
                  try:
                      with open(pylint_file, 'r', encoding='utf-8', errors='replace') as f:
                          for line in f:
                              match = re.match(r'^(.+?):(\d+):\d+:\s*([CRWEF]\d+)\(([^)]+)\)\s*(.+)$', line.strip())
                              if match:
                                  filepath = match.group(1)
                                  lineno = match.group(2)
                                  code = match.group(3)
                                  code_name = match.group(4)
                                  message = match.group(5)
          
                                  parts = filepath.replace('\\', '/').split('/')
                                  if len(parts) > 2:
                                      filepath = '/'.join(parts[-2:])
          
                                  if len(message) > 60:
                                      message = message[:57] + '...'
          
                                  pylint_errors.append({
                                      'file': filepath,
                                      'line': lineno,
                                      'message': f"[{code}] {message}",
                                      'check': f"Pylint: {code_name}"
                                  })
                  except FileNotFoundError:
                      pass
          except Exception as e:
              print(f"Error parsing pylint output: {e}")
          
          # Store pylint errors separately
          result['pylint_errors'] = pylint_errors[:20]
          result['pylint_errors_count'] = len(pylint_errors)
          
          # Also parse Ruff errors if they exist
          ruff_errors = []
          try:
              for ruff_file in ['reports/ruff-changed.txt']:
                  try:
                      with open(ruff_file, 'r', encoding='utf-8', errors='replace') as f:
                          for line in f:
                              match = re.match(r'^(.+?):(\d+):(\d+):\s*([A-Z]+\d+)\s+(.+)$', line.strip())
                              if match:
                                  filepath = match.group(1)
                                  lineno = match.group(2)
                                  code = match.group(4)
                                  message = match.group(5)
          
                                  filepath = re.sub(r'/home/runner/work/[^/]+/', '', filepath)
                                  parts = filepath.replace('\\', '/').split('/')
                                  if len(parts) > 2:
                                      filepath = '/'.join(parts[-2:])
          
                                  if len(message) > 60:
                                      message = message[:57] + '...'
          
                                  ruff_errors.append({
                                      'file': filepath,
                                      'line': lineno,
                                      'message': f"[{code}] {message}",
                                      'check': f"Ruff: {code}"
                                  })
                  except FileNotFoundError:
                      pass
          except Exception as e:
              print(f"Error parsing ruff output: {e}")
          
          result['ruff_errors'] = ruff_errors[:20]
          result['ruff_errors_count'] = len(ruff_errors)
          
          # Re-save with all data
          with open('reports/extracted_issues.json', 'w') as f:
              json.dump(result, f, indent=2)
          
          if pylint_errors:
              print(f"Also found {len(pylint_errors)} Pylint errors")
          if ruff_errors:
              print(f"Also found {len(ruff_errors)} Ruff errors")
          PYEOF

      - name: Upload reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: validation-reports
          path: reports/
          retention-days: 7

      - name: Comment PR with report
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          MODULE_COUNT: ${{ steps.find-modules.outputs.count }}
          EXIT_CODE: ${{ steps.validate.outputs.exit_code }}
          DURATION: ${{ steps.validate.outputs.duration }}
          SCOPE: ${{ inputs.validation-scope }}
          ERRORS_COUNT: ${{ steps.summary.outputs.errors }}
          WARNINGS_COUNT: ${{ steps.summary.outputs.warnings }}
          DOCSTRING_COV: ${{ steps.metrics.outputs.docstring_cov }}
          DOCSTRING_DOCUMENTED: ${{ steps.metrics.outputs.docstring_documented }}
          DOCSTRING_TOTAL: ${{ steps.metrics.outputs.docstring_total }}
          STRING_COV: ${{ steps.metrics.outputs.string_cov }}
          STRING_DOCUMENTED: ${{ steps.metrics.outputs.string_documented }}
          STRING_TOTAL: ${{ steps.metrics.outputs.string_total }}
          HELP_COV: ${{ steps.metrics.outputs.help_cov }}
          HELP_DOCUMENTED: ${{ steps.metrics.outputs.help_documented }}
          HELP_TOTAL: ${{ steps.metrics.outputs.help_total }}
          MISSING_DOCSTRING_PR: ${{ steps.metrics.outputs.missing_docstring_pr }}
          MISSING_STRING_PR: ${{ steps.metrics.outputs.missing_string_pr }}
          MISSING_HELP_PR: ${{ steps.metrics.outputs.missing_help_pr }}
          RUFF_CHANGED: ${{ steps.ruff.outputs.changed }}
          RUFF_FULL: ${{ steps.ruff.outputs.full }}
          PYLINT_CHANGED: ${{ steps.pylint.outputs.changed }}
          PYLINT_FULL: ${{ steps.pylint.outputs.full }}
          DOCSTRING_THRESHOLD: ${{ inputs.docstring-threshold }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          FAIL_ON_WARNINGS: ${{ inputs.fail-on-warnings }}
          GITHUB_SERVER_URL: ${{ github.server_url }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_RUN_ID: ${{ github.run_id }}
        with:
          script: |
            const fs = require('fs');

            function safeInt(val, def = 0) {
              const parsed = parseInt(val);
              return isNaN(parsed) ? def : parsed;
            }

            const prNumber = safeInt(process.env.PR_NUMBER);
            if (!prNumber) {
              console.log('No PR number found, skipping comment');
              return;
            }

            // Build workflow URL
            const serverUrl = process.env.GITHUB_SERVER_URL || 'https://github.com';
            const repository = process.env.GITHUB_REPOSITORY || '';
            const runId = process.env.GITHUB_RUN_ID || '';
            const workflowUrl = `${serverUrl}/${repository}/actions/runs/${runId}`;

            const moduleCount = safeInt(process.env.MODULE_COUNT);
            const exitCode = safeInt(process.env.EXIT_CODE);
            const duration = safeInt(process.env.DURATION);
            const scope = process.env.SCOPE || 'changed';
            const threshold = safeInt(process.env.DOCSTRING_THRESHOLD, 80);
            const errorsCount = safeInt(process.env.ERRORS_COUNT);
            const warningsCount = safeInt(process.env.WARNINGS_COUNT);
            const failOnWarnings = process.env.FAIL_ON_WARNINGS === 'true';

            const docstringCov = safeInt(process.env.DOCSTRING_COV);
            const documentedMethods = safeInt(process.env.DOCSTRING_DOCUMENTED);
            const totalMethods = safeInt(process.env.DOCSTRING_TOTAL, 1);

            const stringCov = safeInt(process.env.STRING_COV);
            const fieldsWithString = safeInt(process.env.STRING_DOCUMENTED);
            const totalFields = safeInt(process.env.STRING_TOTAL, 1);

            const helpCov = safeInt(process.env.HELP_COV);
            const fieldsWithHelp = safeInt(process.env.HELP_DOCUMENTED);
            const helpTotal = safeInt(process.env.HELP_TOTAL, 1);

            const missingDocstringPr = safeInt(process.env.MISSING_DOCSTRING_PR);
            const missingStringPr = safeInt(process.env.MISSING_STRING_PR);
            const missingHelpPr = safeInt(process.env.MISSING_HELP_PR);

            const ruffChanged = safeInt(process.env.RUFF_CHANGED);
            const ruffFull = safeInt(process.env.RUFF_FULL);
            const pylintChanged = safeInt(process.env.PYLINT_CHANGED);
            const pylintFull = safeInt(process.env.PYLINT_FULL);
            
            // Debug Ruff values
            console.log(`Ruff values - Changed: ${ruffChanged}, Full: ${ruffFull}`);

            // Load extracted issues
            let extractedIssues = { errors: [], warnings: [], total_errors: 0, total_warnings: 0 };
            try {
              const issuesJson = fs.readFileSync('reports/extracted_issues.json', 'utf8');
              extractedIssues = JSON.parse(issuesJson);
            } catch (e) {
              console.log('Could not load extracted issues:', e.message);
            }

            // Use totals from extracted JSON if available
            const totalErrors = extractedIssues.total_errors || errorsCount;
            const totalWarnings = extractedIssues.total_warnings || warningsCount;

            function icon(val, th, reverse = false) {
              if (reverse) return val <= th ? ':white_check_mark:' : ':x:';
              return val >= th ? ':white_check_mark:' : ':warning:';
            }

            // Determine final status
            const hasBlockingErrors = exitCode !== 0;
            const hasRuffIssues = ruffChanged > 0;
            const hasPylintIssues = pylintChanged > 0;
            const hasWarnings = warningsCount > 0 || hasRuffIssues || hasPylintIssues;
            
            const hasFailed = hasBlockingErrors || (failOnWarnings && hasWarnings);
            
            const overallStatus = hasFailed ? ':x: Failed' : ':white_check_mark: Passed';
            
            // Determine failure reason
            let failureReason = '';
            if (hasBlockingErrors) {
              failureReason = 'blocking errors from solt-check-odoo';
            } else if (failOnWarnings && hasRuffIssues) {
              failureReason = `Ruff issues (${ruffChanged}) with fail-on-warnings enabled`;
            } else if (failOnWarnings && hasPylintIssues) {
              failureReason = `Pylint issues (${pylintChanged}) with fail-on-warnings enabled`;
            } else if (failOnWarnings && warningsCount > 0) {
              failureReason = `warnings (${warningsCount}) with fail-on-warnings enabled`;
            }

            // Build the comment body
            const bodyParts = [];

            // Header
            bodyParts.push('## :bar_chart: Validation Report');
            bodyParts.push('');
            if (hasFailed) {
              bodyParts.push('> :no_entry: **This PR is blocked.** Fix the errors below before merging.');
            } else {
              bodyParts.push('> :white_check_mark: **All checks passed.** This PR is ready for review.');
            }
            bodyParts.push('');

            // Show first 10 blocking errors directly, link to workflow for all
            const hasExtractedErrors = extractedIssues.errors && extractedIssues.errors.length > 0;
            const hasExtractedWarnings = extractedIssues.warnings && extractedIssues.warnings.length > 0;
            const MAX_DISPLAY = 10;
            
            if (hasFailed) {
              if (hasExtractedErrors) {
                bodyParts.push(`### :no_entry: Blocking Errors (${totalErrors})`);
                bodyParts.push('');
                bodyParts.push('| File | Line | Error |');
                bodyParts.push('|------|------|-------|');

                // Show only first 10 errors
                for (const err of extractedIssues.errors.slice(0, MAX_DISPLAY)) {
                  const file = err.file ? `\`${err.file}\`` : '-';
                  const line = err.line || '-';
                  const msg = err.message || 'Unknown error';
                  bodyParts.push(`| ${file} | ${line} | ${msg} |`);
                }

                bodyParts.push('');
                if (totalErrors > MAX_DISPLAY) {
                  bodyParts.push(`> :mag: Showing ${MAX_DISPLAY} of ${totalErrors} errors. **[View all errors in workflow logs](${workflowUrl})**`);
                  bodyParts.push('');
                }
              }
              
              // Show warnings as blocking when fail-on-warnings is enabled
              if (failOnWarnings && hasExtractedWarnings) {
                bodyParts.push(`### :warning: Blocking Warnings (${totalWarnings})`);
                bodyParts.push('');
                bodyParts.push('> **Note:** `fail-on-warnings` is enabled. Warnings are treated as blocking issues.');
                bodyParts.push('');
                bodyParts.push('| File | Line | Warning |');
                bodyParts.push('|------|------|---------|');

                // Show only first 10 warnings
                for (const warn of extractedIssues.warnings.slice(0, MAX_DISPLAY)) {
                  const file = warn.file ? `\`${warn.file}\`` : '-';
                  const line = warn.line || '-';
                  const msg = warn.message || 'Unknown warning';
                  bodyParts.push(`| ${file} | ${line} | ${msg} |`);
                }

                bodyParts.push('');
                if (totalWarnings > MAX_DISPLAY) {
                  bodyParts.push(`> :mag: Showing ${MAX_DISPLAY} of ${totalWarnings} warnings. **[View all warnings in workflow logs](${workflowUrl})**`);
                  bodyParts.push('');
                }
              }
              
              // If failed due to Ruff/Pylint issues
              const ruffErrors = extractedIssues.ruff_errors || [];
              if (failOnWarnings && (hasRuffIssues || hasPylintIssues)) {
                bodyParts.push('### :wrench: Code Quality Issues');
                bodyParts.push('');
                
                if (hasRuffIssues && ruffErrors.length > 0) {
                  bodyParts.push(`#### Ruff (${ruffChanged} issues in PR)`);
                  bodyParts.push('');
                  bodyParts.push('| File | Line | Error |');
                  bodyParts.push('|------|------|-------|');
                  for (const err of ruffErrors.slice(0, MAX_DISPLAY)) {
                    const file = err.file ? `\`${err.file}\`` : '-';
                    const line = err.line || '-';
                    const msg = err.message || 'Unknown error';
                    bodyParts.push(`| ${file} | ${line} | ${msg} |`);
                  }
                  bodyParts.push('');
                  if (ruffChanged > MAX_DISPLAY) {
                    bodyParts.push(`> Showing ${MAX_DISPLAY} of ${ruffChanged} Ruff issues. **[View all in workflow logs](${workflowUrl})**`);
                    bodyParts.push('');
                  }
                } else if (hasRuffIssues) {
                  bodyParts.push(`- **Ruff:** ${ruffChanged} issues in changed files`);
                }
                
                if (hasPylintIssues) {
                  bodyParts.push(`- **Pylint:** ${pylintChanged} issues in changed files`);
                  bodyParts.push('');
                }
              }
            }

            // Summary section
            bodyParts.push('### :clipboard: Summary');
            bodyParts.push('');
            bodyParts.push('| Metric | Value |');
            bodyParts.push('|--------|-------|');
            bodyParts.push(`| Status | ${overallStatus} |`);
            bodyParts.push(`| Modules | ${moduleCount} |`);
            bodyParts.push(`| Scope | \`${scope}\` |`);
            bodyParts.push(`| Time | ${duration}s |`);
            bodyParts.push('');

            // Documentation Coverage
            bodyParts.push('### :books: Documentation Coverage');
            bodyParts.push('');
            bodyParts.push('| Metric | Coverage | Detail | Goal | Status |');
            bodyParts.push('|--------|----------|--------|------|--------|');
            bodyParts.push(`| Docstrings | **${docstringCov}%** | ${documentedMethods}/${totalMethods} | >=${threshold}% | ${icon(docstringCov, threshold)} |`);
            bodyParts.push(`| Field strings | **${stringCov}%** | ${fieldsWithString}/${totalFields} | >=90% | ${icon(stringCov, 90)} |`);
            bodyParts.push(`| Field help | **${helpCov}%** | ${fieldsWithHelp}/${helpTotal} | >=50% | ${icon(helpCov, 50)} |`);
            bodyParts.push('');

            // Issues in this PR
            bodyParts.push('### :warning: Issues in this PR');
            bodyParts.push('');
            bodyParts.push('| Type | Count |');
            bodyParts.push('|------|-------|');
            bodyParts.push(`| Missing docstrings | ${missingDocstringPr} |`);
            bodyParts.push(`| Fields without string | ${missingStringPr} |`);
            bodyParts.push(`| Fields without help | ${missingHelpPr} |`);
            bodyParts.push('');

            // Code Quality
            bodyParts.push('### :mag: Code Quality');
            bodyParts.push('');
            bodyParts.push('| Tool | PR | Repo | Status |');
            bodyParts.push('|------|-----|------|--------|');
            bodyParts.push(`| Ruff | ${ruffChanged} | ${ruffFull} | ${icon(ruffFull, 0, true)} |`);
            bodyParts.push('');

            // Footer
            bodyParts.push('---');
            if (hasFailed) {
              if (failureReason) {
                bodyParts.push(`:no_entry: **Validation failed due to:** ${failureReason}`);
                bodyParts.push('');
              }
              bodyParts.push(':bulb: **Tip:** Fix the issues above and push again.');
              bodyParts.push('');
            }
            bodyParts.push(':robot: *solt-pre-commit*');

            const body = bodyParts.join('\n');

            console.log(`Posting comment to PR #${prNumber}`);

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && 
              (c.body.includes('Validation Failed') || c.body.includes('Validation Passed') || c.body.includes('Validation Report'))
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
              console.log('Updated existing comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: body
              });
              console.log('Created new comment');
            }

      # CRITICAL: This step determines the final job status
      # It runs AFTER all other steps and will FAIL the job if there are blocking issues
      - name: Final validation check
        id: final-check
        if: always()
        run: |
          EXIT_CODE="${{ steps.validate.outputs.exit_code }}"
          EXIT_CODE=$(echo "$EXIT_CODE" | grep -oE '^[0-9]+' || echo 0)
          if [ -z "$EXIT_CODE" ]; then EXIT_CODE=0; fi

          FAIL_ON_WARNINGS="${{ inputs.fail-on-warnings }}"
          WARNINGS="${{ steps.summary.outputs.warnings }}"
          WARNINGS=$(echo "$WARNINGS" | grep -oE '^[0-9]+' || echo 0)
          
          RUFF_CHANGED="${{ steps.ruff.outputs.changed }}"
          RUFF_CHANGED=$(echo "$RUFF_CHANGED" | grep -oE '^[0-9]+' || echo 0)
          if [ -z "$RUFF_CHANGED" ]; then RUFF_CHANGED=0; fi

          echo "::notice::Final check - Exit code: $EXIT_CODE, Warnings: $WARNINGS, Ruff (changed): $RUFF_CHANGED, Fail on warnings: $FAIL_ON_WARNINGS"

          # Determine final result
          if [ "$EXIT_CODE" -ne 0 ]; then
            echo "result=failure" >> $GITHUB_OUTPUT
            echo "::error::Validation FAILED with blocking issues (exit code: $EXIT_CODE)"
            exit 1
          elif [ "$FAIL_ON_WARNINGS" = "true" ] && [ "$WARNINGS" -gt 0 ]; then
            echo "result=failure" >> $GITHUB_OUTPUT
            echo "::error::Validation FAILED due to warnings (fail-on-warnings enabled)"
            exit 1
          elif [ "$FAIL_ON_WARNINGS" = "true" ] && [ "$RUFF_CHANGED" -gt 0 ]; then
            echo "result=failure" >> $GITHUB_OUTPUT
            echo "::error::Validation FAILED due to Ruff issues in changed files ($RUFF_CHANGED issues, fail-on-warnings enabled)"
            exit 1
          else
            echo "result=success" >> $GITHUB_OUTPUT
            echo "::notice::Validation PASSED"
            exit 0
          fi

  # ---------------------------------------------------------------------------
  # UPDATE BADGES (on push to protected branches)
  # ---------------------------------------------------------------------------
  update-badges:
    name: Update Documentation Badges
    runs-on: ubuntu-latest
    needs: validate
    if: |
      github.event_name == 'push' &&
      (github.ref == 'refs/heads/main' ||
       github.ref == 'refs/heads/master' ||
       github.ref == 'refs/heads/develop' ||
       startsWith(github.ref, 'refs/heads/17.0') ||
       startsWith(github.ref, 'refs/heads/18.0'))

    steps:
      - name: Update docstrings badge
        if: inputs.gist-id != ''
        uses: schneegans/dynamic-badges-action@v1.7.0
        continue-on-error: true
        with:
          auth: ${{ secrets.GIST_SECRET }}
          gistID: ${{ inputs.gist-id }}
          filename: ${{ inputs.badge-filename-prefix }}-docstrings.json
          label: docstrings
          message: "${{ needs.validate.outputs.docstring_cov }}%"
          valColorRange: ${{ needs.validate.outputs.docstring_cov }}
          minColorRange: 0
          maxColorRange: 100
          namedLogo: python
          logoColor: white

      - name: Update docs status badge
        if: inputs.gist-id != ''
        uses: schneegans/dynamic-badges-action@v1.7.0
        continue-on-error: true
        with:
          auth: ${{ secrets.GIST_SECRET }}
          gistID: ${{ inputs.gist-id }}
          filename: ${{ inputs.badge-filename-prefix }}-docs-status.json
          label: Documentation Standards
          message: ${{ needs.validate.outputs.docs_status }}
          color: ${{ needs.validate.outputs.docs_status == 'passing' && 'brightgreen' || 'red' }}
          namedLogo: github
          logoColor: white

  # ---------------------------------------------------------------------------
  # PRE-COMMIT HOOKS
  # ---------------------------------------------------------------------------
  pre-commit:
    name: Pre-commit Hooks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install pre-commit
        run: pip install pre-commit

      - name: Get changed files
        id: changed
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            git fetch origin ${{ github.base_ref }} --depth=1
            CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | tr '\n' ' ')
          else
            CHANGED_FILES=$(git diff --name-only HEAD~1...HEAD 2>/dev/null | tr '\n' ' ')
          fi

          VALID_FILES=""
          for f in $CHANGED_FILES; do
            if [ -f "$f" ]; then
              VALID_FILES="$VALID_FILES $f"
            fi
          done
          VALID_FILES=$(echo "$VALID_FILES" | xargs)

          FILE_COUNT=$(echo "$VALID_FILES" | wc -w | tr -d ' ')
          if [ -z "$FILE_COUNT" ]; then FILE_COUNT=0; fi

          echo "files=$VALID_FILES" >> $GITHUB_OUTPUT
          echo "count=$FILE_COUNT" >> $GITHUB_OUTPUT

      - name: Run pre-commit
        id: precommit
        run: |
          FILES="${{ steps.changed.outputs.files }}"
          COUNT="${{ steps.changed.outputs.count }}"
          if [ -z "$COUNT" ]; then COUNT=0; fi

          if [ -z "$FILES" ] || [ "$COUNT" = "0" ]; then
            echo "No files to check"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Checking $COUNT files..."
          if pre-commit run --files $FILES 2>&1 | tee precommit_output.txt; then
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Pre-commit Hooks" >> $GITHUB_STEP_SUMMARY
          echo "Files checked: ${{ steps.changed.outputs.count }}" >> $GITHUB_STEP_SUMMARY
          STATUS="${{ steps.precommit.outputs.status }}"
          if [ "$STATUS" = "passed" ]; then
            echo "All hooks passed" >> $GITHUB_STEP_SUMMARY
          elif [ "$STATUS" = "skipped" ]; then
            echo "No files to check" >> $GITHUB_STEP_SUMMARY
          else
            echo "Some hooks failed" >> $GITHUB_STEP_SUMMARY
          fi
