# =============================================================================
# SOLT PRE-COMMIT - Reusable Workflow
# =============================================================================
# IMPORTANT: This workflow will FAIL (red) when blocking issues are found.
# The validation job exit code is properly propagated.
# =============================================================================
name: Solt Validation (Reusable)

on:
  workflow_call:
    inputs:
      python-version:
        description: 'Python version to use'
        required: false
        type: string
        default: '3.11'
      odoo-version:
        description: 'Odoo version (for compatibility checks)'
        required: false
        type: string
        default: '17.0'
      validation-scope:
        description: 'Validation scope: changed or full'
        required: false
        type: string
        default: 'changed'
      run-coverage:
        description: 'Run coverage analysis'
        required: false
        type: boolean
        default: false
      coverage-threshold:
        description: 'Minimum coverage percentage'
        required: false
        type: number
        default: 60
      fail-on-warnings:
        description: 'Fail if warnings are found'
        required: false
        type: boolean
        default: false
      show-info:
        description: 'Show info-level issues in report'
        required: false
        type: boolean
        default: false
      gist-id:
        description: 'Gist ID for badge storage (optional)'
        required: false
        type: string
        default: ''
      badge-filename-prefix:
        description: 'Prefix for badge filenames in gist (e.g., solt-budget)'
        required: false
        type: string
        default: ''
      docstring-threshold:
        description: 'Minimum docstring coverage to pass (default: 80)'
        required: false
        type: number
        default: 80
    secrets:
      GIST_SECRET:
        description: 'GitHub token for updating gist badges'
        required: false
    outputs:
      validation-result:
        description: 'Validation result (success/failure)'
        value: ${{ jobs.validate.outputs.result }}
      errors-count:
        description: 'Number of errors found'
        value: ${{ jobs.validate.outputs.errors }}
      warnings-count:
        description: 'Number of warnings found'
        value: ${{ jobs.validate.outputs.warnings }}
      docstring-coverage:
        description: 'Docstring coverage percentage'
        value: ${{ jobs.validate.outputs.docstring_cov }}

jobs:
  # ---------------------------------------------------------------------------
  # BRANCH VALIDATION
  # ---------------------------------------------------------------------------
  branch-check:
    name: Branch Name
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}
      - name: Install solt-pre-commit
        run: pip install "git+https://github.com/soltein-net/solt-pre-commit.git@v1.0.0"
      - name: Validate branch name
        run: |
          BRANCH="${{ github.head_ref }}"
          echo "## Branch Validation" >> $GITHUB_STEP_SUMMARY
          echo "Branch: \`$BRANCH\`" >> $GITHUB_STEP_SUMMARY
          if solt-check-branch "$BRANCH"; then
            echo "Branch name is valid" >> $GITHUB_STEP_SUMMARY
          else
            echo "Branch name is invalid" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

  # ---------------------------------------------------------------------------
  # MAIN VALIDATION
  # ---------------------------------------------------------------------------
  validate:
    name: Odoo Module Validation
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    outputs:
      result: ${{ steps.final-check.outputs.result }}
      errors: ${{ steps.summary.outputs.errors }}
      warnings: ${{ steps.summary.outputs.warnings }}
      docstring_cov: ${{ steps.metrics.outputs.docstring_cov }}
      docs_status: ${{ steps.metrics.outputs.docs_status }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install "git+https://github.com/soltein-net/solt-pre-commit.git@v1.0.0"
          pip install pylint-odoo ruff

      - name: Find Odoo modules
        id: find-modules
        run: |
          MODULES=$(find . -name "__manifest__.py" -o -name "__openerp__.py" | xargs -I {} dirname {} | sort -u | tr '\n' ' ')
          MODULE_COUNT=$(echo "$MODULES" | wc -w | tr -d ' ')
          if [ -z "$MODULE_COUNT" ] || [ "$MODULE_COUNT" = "" ]; then
            MODULE_COUNT=0
          fi
          echo "modules=$MODULES" >> $GITHUB_OUTPUT
          echo "count=$MODULE_COUNT" >> $GITHUB_OUTPUT
          echo "Found $MODULE_COUNT modules: $MODULES"

      - name: Run validation (changed files)
        id: validate
        run: |
          mkdir -p reports
          ARGS="--scope ${{ inputs.validation-scope }}"
          if [ "${{ inputs.show-info }}" = "true" ]; then
            ARGS="$ARGS --show-info"
          fi

          START_TIME=$(date +%s)

          # Run validation and capture exit code
          set +e
          solt-check-odoo ${{ steps.find-modules.outputs.modules }} $ARGS 2>&1 | tee reports/validation.txt
          VALIDATION_EXIT_CODE=$?
          set -e

          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))

          echo "exit_code=$VALIDATION_EXIT_CODE" >> $GITHUB_OUTPUT
          echo "duration=$DURATION" >> $GITHUB_OUTPUT

          # Show exit code in logs for debugging
          echo "::notice::Validation exit code: $VALIDATION_EXIT_CODE"

      - name: Run full validation for metrics
        continue-on-error: true
        run: |
          solt-check-odoo ${{ steps.find-modules.outputs.modules }} --scope full --show-info 2>&1 | tee reports/validation-full.txt || true

      - name: Run Ruff check
        id: ruff
        continue-on-error: true
        run: |
          set +e
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            git fetch origin ${{ github.base_ref }} --depth=1 2>/dev/null || true
            CHANGED_PY=$(git diff --name-only origin/${{ github.base_ref }}...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          else
            CHANGED_PY=$(git diff --name-only HEAD~1...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          fi

          RUFF_CHANGED=0
          if [ -n "$CHANGED_PY" ]; then
            ruff check $CHANGED_PY --output-format=concise 2>&1 | tee reports/ruff-changed.txt || true
            RUFF_CHANGED=$(grep -cE "^.+:[0-9]+:[0-9]+:" reports/ruff-changed.txt 2>/dev/null || echo 0)
          fi

          ruff check ${{ steps.find-modules.outputs.modules }} --output-format=concise 2>&1 | tee reports/ruff-full.txt || true
          RUFF_FULL=$(grep -cE "^.+:[0-9]+:[0-9]+:" reports/ruff-full.txt 2>/dev/null || echo 0)

          RUFF_CHANGED=$(echo "$RUFF_CHANGED" | grep -oE '^[0-9]+' || echo 0)
          RUFF_FULL=$(echo "$RUFF_FULL" | grep -oE '^[0-9]+' || echo 0)

          echo "changed=${RUFF_CHANGED:-0}" >> $GITHUB_OUTPUT
          echo "full=${RUFF_FULL:-0}" >> $GITHUB_OUTPUT

      - name: Run Pylint-Odoo check
        id: pylint
        continue-on-error: true
        run: |
          set +e
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            CHANGED_PY=$(git diff --name-only origin/${{ github.base_ref }}...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          else
            CHANGED_PY=$(git diff --name-only HEAD~1...HEAD -- '*.py' 2>/dev/null | tr '\n' ' ')
          fi

          PYLINT_CHANGED=0
          if [ -n "$CHANGED_PY" ]; then
            echo "$CHANGED_PY" | xargs pylint --load-plugins=pylint_odoo --exit-zero 2>&1 | tee reports/pylint-changed.txt || true
            PYLINT_CHANGED=$(grep -cE "^[CRWEF][0-9]{4}:" reports/pylint-changed.txt 2>/dev/null || echo 0)
          fi

          find ${{ steps.find-modules.outputs.modules }} -name "*.py" -not -path "*/__pycache__/*" 2>/dev/null | head -100 | xargs pylint --load-plugins=pylint_odoo --exit-zero 2>&1 | tee reports/pylint-full.txt || true
          PYLINT_FULL=$(grep -cE "^[CRWEF][0-9]{4}:" reports/pylint-full.txt 2>/dev/null || echo 0)

          PYLINT_CHANGED=$(echo "$PYLINT_CHANGED" | grep -oE '^[0-9]+' || echo 0)
          PYLINT_FULL=$(echo "$PYLINT_FULL" | grep -oE '^[0-9]+' || echo 0)

          echo "changed=${PYLINT_CHANGED:-0}" >> $GITHUB_OUTPUT
          echo "full=${PYLINT_FULL:-0}" >> $GITHUB_OUTPUT

      - name: Calculate real metrics
        id: metrics
        if: always()
        env:
          DOCSTRING_THRESHOLD: ${{ inputs.docstring-threshold }}
        run: |
          # Debug: Show if validation files exist
          echo "=== Debug: Checking report files ==="
          ls -la reports/ 2>/dev/null || echo "reports/ directory not found"
          
          if [ -f "reports/validation-full.txt" ]; then
            echo "=== Last 30 lines of validation-full.txt ==="
            tail -30 reports/validation-full.txt
            echo "=== Searching for METRICS: line ==="
            grep "METRICS:" reports/validation-full.txt || echo "METRICS: line not found"
          else
            echo "validation-full.txt not found"
          fi
          
          python3 << 'EOF'
          import os
          import re
          import json

          print("=== Python metrics script starting ===")

          metrics_data = {
              'docstring_cov': 0, 'docstring_documented': 0, 'docstring_total': 1,
              'string_cov': 0, 'string_documented': 0, 'string_total': 1,
              'help_cov': 0, 'help_documented': 0, 'help_total': 1,
              'models': 0
          }

          try:
              with open('reports/validation-full.txt', 'r') as f:
                  content = f.read()
              print(f"Read {len(content)} bytes from validation-full.txt")
          
              metrics_match = re.search(r'^METRICS:(.+)$', content, re.MULTILINE)
              if metrics_match:
                  print(f"Found METRICS line: {metrics_match.group(1)[:100]}...")
                  for pair in metrics_match.group(1).split(','):
                      if '=' in pair:
                          key, value = pair.split('=', 1)
                          try:
                              metrics_data[key.strip()] = float(value) if '.' in value else int(value)
                          except ValueError:
                              print(f"  Failed to parse: {pair}")
              else:
                  print("METRICS: line NOT found in file")
                  # Try to find any line containing METRICS
                  for line in content.split('\n'):
                      if 'METRICS' in line or 'metrics' in line.lower():
                          print(f"  Found similar line: {line[:100]}")
          except FileNotFoundError:
              print("ERROR: validation-full.txt not found")
          except Exception as e:
              print(f"ERROR reading validation-full.txt: {e}")

          missing_docstring_pr = missing_string_pr = missing_help_pr = 0
          try:
              with open('reports/validation.txt', 'r') as f:
                  pr_content = f.read().lower()
              missing_docstring_pr = pr_content.count('missing docstring')
              missing_string_pr = pr_content.count('missing string')
              missing_help_pr = pr_content.count('missing help')
          except FileNotFoundError:
              pass

          docstring_cov = metrics_data.get('docstring_cov', 0)
          threshold = int(os.environ.get('DOCSTRING_THRESHOLD', 80))
          docs_status = 'passing' if docstring_cov >= threshold else 'failing'

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"docstring_cov={int(docstring_cov)}\n")
              f.write(f"docstring_documented={int(metrics_data.get('docstring_documented', 0))}\n")
              f.write(f"docstring_total={int(metrics_data.get('docstring_total', 1))}\n")
              f.write(f"string_cov={int(metrics_data.get('string_cov', 0))}\n")
              f.write(f"string_documented={int(metrics_data.get('string_documented', 0))}\n")
              f.write(f"string_total={int(metrics_data.get('string_total', 1))}\n")
              f.write(f"help_cov={int(metrics_data.get('help_cov', 0))}\n")
              f.write(f"help_documented={int(metrics_data.get('help_documented', 0))}\n")
              f.write(f"help_total={int(metrics_data.get('help_total', 1))}\n")
              f.write(f"models={int(metrics_data.get('models', 0))}\n")
              f.write(f"missing_docstring_pr={missing_docstring_pr}\n")
              f.write(f"missing_string_pr={missing_string_pr}\n")
              f.write(f"missing_help_pr={missing_help_pr}\n")
              f.write(f"docs_status={docs_status}\n")

          with open('reports/metrics.json', 'w') as f:
              json.dump(metrics_data, f, indent=2)

          print(f"=== Final metrics ===")
          print(f"Docstrings: {docstring_cov:.1f}% ({metrics_data.get('docstring_documented', 0)}/{metrics_data.get('docstring_total', 1)})")
          print(f"Field strings: {metrics_data.get('string_cov', 0):.1f}%")
          print(f"Field help: {metrics_data.get('help_cov', 0):.1f}%")
          print(f"Status: {docs_status}")
          EOF

          echo "ruff_changed=${{ steps.ruff.outputs.changed }}" >> $GITHUB_OUTPUT
          echo "ruff_full=${{ steps.ruff.outputs.full }}" >> $GITHUB_OUTPUT
          echo "pylint_changed=${{ steps.pylint.outputs.changed }}" >> $GITHUB_OUTPUT
          echo "pylint_full=${{ steps.pylint.outputs.full }}" >> $GITHUB_OUTPUT

      - name: Generate summary and extract errors
        id: summary
        if: always()
        run: |
          EXIT_CODE="${{ steps.validate.outputs.exit_code }}"
          EXIT_CODE=$(echo "$EXIT_CODE" | grep -oE '^[0-9]+' || echo 0)
          if [ -z "$EXIT_CODE" ]; then EXIT_CODE=0; fi

          ERRORS=$(grep -ciE "^\[ERROR\]" reports/validation.txt 2>/dev/null || echo 0)
          WARNINGS=$(grep -ciE "^\[WARN\]" reports/validation.txt 2>/dev/null || echo 0)

          echo "errors=$ERRORS" >> $GITHUB_OUTPUT
          echo "warnings=$WARNINGS" >> $GITHUB_OUTPUT
          echo "validation_exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT

          # Extract top errors for PR comment
          python3 << 'PYEOF'
          import os
          import re
          import json

          errors_list = []
          warnings_list = []
          total_errors = 0
          total_warnings = 0

          try:
              with open('reports/validation.txt', 'r', encoding='utf-8', errors='replace') as f:
                  content = f.read()

              # Output format from solt-check-odoo:
              # [ERROR] ERRORS (3) [BLOCKING]
              # --------------------------------------------------
              #
              #   Python Duplicate Field Label (1)
              #     - /path/to/file.py:45 Fields (amount, total) have same label
              #
              # [WARN] WARNINGS (5)
              # --------------------------------------------------
              #
              #   Python Field Missing String (2)
              #     - models/budget.py:67 Field "state" is missing string attribute

              current_severity = None
              current_check = None

              for line in content.split('\n'):
                  line_stripped = line.strip()

                  # Detect severity section headers
                  if line_stripped.startswith('[ERROR]'):
                      current_severity = 'error'
                      # Extract count from header like "[ERROR] ERRORS (3)"
                      match = re.search(r'\((\d+)\)', line_stripped)
                      if match:
                          total_errors = int(match.group(1))
                      continue
                  elif line_stripped.startswith('[WARN]'):
                      current_severity = 'warning'
                      match = re.search(r'\((\d+)\)', line_stripped)
                      if match:
                          total_warnings = int(match.group(1))
                      continue
                  elif line_stripped.startswith('[INFO]'):
                      current_severity = 'info'
                      continue

                  # Skip separators and empty lines
                  if not line_stripped or line_stripped.startswith('---') or line_stripped.startswith('==='):
                      continue

                  # Detect check name headers like "  Python Field Missing String (2)"
                  check_header_match = re.match(r'^  ([A-Z][^(]+)\s*\((\d+)\)', line)
                  if check_header_match:
                      current_check = check_header_match.group(1).strip()
                      continue

                  # Parse error/warning lines starting with "    - "
                  if line.startswith('    - ') and current_severity in ('error', 'warning'):
                      msg_content = line[6:]  # Remove "    - "

                      # Try to extract file:line pattern
                      # Pattern: /path/to/file.py:123 Message here
                      # Or: file.py:123 Message here
                      file_line_match = re.match(r'^([^\s:]+):(\d+)\s+(.+)$', msg_content)

                      if file_line_match:
                          filepath = file_line_match.group(1)
                          lineno = file_line_match.group(2)
                          message = file_line_match.group(3)

                          # Shorten filepath - keep last 2 parts
                          parts = filepath.replace('\\', '/').split('/')
                          if len(parts) > 2:
                              filepath = '/'.join(parts[-2:])

                          # Truncate long messages
                          if len(message) > 80:
                              message = message[:77] + '...'

                          entry = {
                              'file': filepath,
                              'line': lineno,
                              'message': message,
                              'check': current_check
                          }
                      else:
                          # No file:line pattern, just message
                          message = msg_content
                          if len(message) > 100:
                              message = message[:97] + '...'
                          entry = {
                              'file': '',
                              'line': '',
                              'message': message,
                              'check': current_check
                          }

                      if current_severity == 'error':
                          errors_list.append(entry)
                      else:
                          warnings_list.append(entry)

          except Exception as e:
              print(f"Error parsing validation output: {e}")
              import traceback
              traceback.print_exc()

          # Limit entries
          errors_list = errors_list[:10]
          warnings_list = warnings_list[:5]

          # Save to JSON for the comment step
          result = {
              'errors': errors_list,
              'warnings': warnings_list,
              'total_errors': total_errors,
              'total_warnings': total_warnings
          }

          with open('reports/extracted_issues.json', 'w') as f:
              json.dump(result, f, indent=2)

          print(f"Extracted {len(errors_list)}/{total_errors} errors, {len(warnings_list)}/{total_warnings} warnings")

          # Debug: show what we extracted
          if errors_list:
              print("Sample errors:")
              for e in errors_list[:3]:
                  print(f"  - {e.get('file')}:{e.get('line')} {e.get('message')}")
          
          # Also parse Pylint errors if they exist
          pylint_errors = []
          try:
              for pylint_file in ['reports/pylint-changed.txt', 'reports/pylint-full.txt']:
                  try:
                      with open(pylint_file, 'r', encoding='utf-8', errors='replace') as f:
                          for line in f:
                              # Pylint format: path/to/file.py:123:0: C0123(code-name) Message
                              match = re.match(r'^(.+?):(\d+):\d+:\s*([CRWEF]\d+)\(([^)]+)\)\s*(.+)$', line.strip())
                              if match:
                                  filepath = match.group(1)
                                  lineno = match.group(2)
                                  code = match.group(3)
                                  code_name = match.group(4)
                                  message = match.group(5)
          
                                  # Shorten filepath
                                  parts = filepath.replace('\\', '/').split('/')
                                  if len(parts) > 2:
                                      filepath = '/'.join(parts[-2:])
          
                                  # Truncate message
                                  if len(message) > 60:
                                      message = message[:57] + '...'
          
                                  pylint_errors.append({
                                      'file': filepath,
                                      'line': lineno,
                                      'message': f"[{code}] {message}",
                                      'check': f"Pylint: {code_name}"
                                  })
                  except FileNotFoundError:
                      pass
          except Exception as e:
              print(f"Error parsing pylint output: {e}")
          
          # Add pylint errors to the main list (up to 5)
          if pylint_errors and len(errors_list) < 10:
              remaining_slots = 10 - len(errors_list)
              errors_list.extend(pylint_errors[:remaining_slots])
              total_errors += len(pylint_errors)
          
          # Update result with pylint errors
          result['errors'] = errors_list[:10]
          result['total_errors'] = total_errors
          result['pylint_errors_count'] = len(pylint_errors)
          
          # Re-save with updated data
          with open('reports/extracted_issues.json', 'w') as f:
              json.dump(result, f, indent=2)
          
          if pylint_errors:
              print(f"Also found {len(pylint_errors)} Pylint errors")
          PYEOF

      - name: Upload reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: validation-reports
          path: reports/
          retention-days: 7

      - name: Comment PR with report
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        env:
          MODULE_COUNT: ${{ steps.find-modules.outputs.count }}
          EXIT_CODE: ${{ steps.validate.outputs.exit_code }}
          DURATION: ${{ steps.validate.outputs.duration }}
          SCOPE: ${{ inputs.validation-scope }}
          ERRORS_COUNT: ${{ steps.summary.outputs.errors }}
          WARNINGS_COUNT: ${{ steps.summary.outputs.warnings }}
          DOCSTRING_COV: ${{ steps.metrics.outputs.docstring_cov }}
          DOCSTRING_DOCUMENTED: ${{ steps.metrics.outputs.docstring_documented }}
          DOCSTRING_TOTAL: ${{ steps.metrics.outputs.docstring_total }}
          STRING_COV: ${{ steps.metrics.outputs.string_cov }}
          STRING_DOCUMENTED: ${{ steps.metrics.outputs.string_documented }}
          STRING_TOTAL: ${{ steps.metrics.outputs.string_total }}
          HELP_COV: ${{ steps.metrics.outputs.help_cov }}
          HELP_DOCUMENTED: ${{ steps.metrics.outputs.help_documented }}
          HELP_TOTAL: ${{ steps.metrics.outputs.help_total }}
          MISSING_DOCSTRING_PR: ${{ steps.metrics.outputs.missing_docstring_pr }}
          MISSING_STRING_PR: ${{ steps.metrics.outputs.missing_string_pr }}
          MISSING_HELP_PR: ${{ steps.metrics.outputs.missing_help_pr }}
          RUFF_CHANGED: ${{ steps.metrics.outputs.ruff_changed }}
          RUFF_FULL: ${{ steps.metrics.outputs.ruff_full }}
          PYLINT_CHANGED: ${{ steps.metrics.outputs.pylint_changed }}
          PYLINT_FULL: ${{ steps.metrics.outputs.pylint_full }}
          DOCSTRING_THRESHOLD: ${{ inputs.docstring-threshold }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
        with:
          script: |
            const fs = require('fs');

            function safeInt(val, def = 0) {
              const parsed = parseInt(val);
              return isNaN(parsed) ? def : parsed;
            }

            const prNumber = safeInt(process.env.PR_NUMBER);
            if (!prNumber) {
              console.log('No PR number found, skipping comment');
              return;
            }

            const moduleCount = safeInt(process.env.MODULE_COUNT);
            const exitCode = safeInt(process.env.EXIT_CODE);
            const duration = safeInt(process.env.DURATION);
            const scope = process.env.SCOPE || 'changed';
            const threshold = safeInt(process.env.DOCSTRING_THRESHOLD, 80);
            const errorsCount = safeInt(process.env.ERRORS_COUNT);
            const warningsCount = safeInt(process.env.WARNINGS_COUNT);

            const docstringCov = safeInt(process.env.DOCSTRING_COV);
            const documentedMethods = safeInt(process.env.DOCSTRING_DOCUMENTED);
            const totalMethods = safeInt(process.env.DOCSTRING_TOTAL, 1);

            const stringCov = safeInt(process.env.STRING_COV);
            const fieldsWithString = safeInt(process.env.STRING_DOCUMENTED);
            const totalFields = safeInt(process.env.STRING_TOTAL, 1);

            const helpCov = safeInt(process.env.HELP_COV);
            const fieldsWithHelp = safeInt(process.env.HELP_DOCUMENTED);
            const helpTotal = safeInt(process.env.HELP_TOTAL, 1);

            const missingDocstringPr = safeInt(process.env.MISSING_DOCSTRING_PR);
            const missingStringPr = safeInt(process.env.MISSING_STRING_PR);
            const missingHelpPr = safeInt(process.env.MISSING_HELP_PR);

            const ruffChanged = safeInt(process.env.RUFF_CHANGED);
            const ruffFull = safeInt(process.env.RUFF_FULL);
            const pylintChanged = safeInt(process.env.PYLINT_CHANGED);
            const pylintFull = safeInt(process.env.PYLINT_FULL);

            // Load extracted issues
            let extractedIssues = { errors: [], warnings: [], total_errors: 0, total_warnings: 0 };
            try {
              const issuesJson = fs.readFileSync('reports/extracted_issues.json', 'utf8');
              extractedIssues = JSON.parse(issuesJson);
            } catch (e) {
              console.log('Could not load extracted issues:', e.message);
            }

            // Use totals from extracted JSON if available, otherwise from grep counts
            const totalErrors = extractedIssues.total_errors || errorsCount;
            const totalWarnings = extractedIssues.total_warnings || warningsCount;

            function icon(val, th, reverse = false) {
              if (reverse) return val <= th ? ':white_check_mark:' : ':x:';
              return val >= th ? ':white_check_mark:' : ':warning:';
            }

            const hasFailed = exitCode !== 0;
            const overallStatus = hasFailed ? ':x: Failed' : ':white_check_mark: Passed';

            // Build the comment body - EXACT format from original image but in English
            const bodyParts = [];

            // Header
            bodyParts.push('## :bar_chart: Validation Report');
            bodyParts.push('');
            bodyParts.push('> :information_source: This analysis is **informational**.');
            bodyParts.push('');

            // If failed, show blocking errors FIRST
            if (hasFailed && extractedIssues.errors && extractedIssues.errors.length > 0) {
              bodyParts.push(`### :no_entry: Blocking Errors (${totalErrors})`);
              bodyParts.push('');
              bodyParts.push('| File | Line | Error |');
              bodyParts.push('|------|------|-------|');

              for (const err of extractedIssues.errors.slice(0, 10)) {
                const file = err.file ? `\`${err.file}\`` : '-';
                const line = err.line || '-';
                const msg = err.message || 'Unknown error';
                bodyParts.push(`| ${file} | ${line} | ${msg} |`);
              }

              if (totalErrors > 10) {
                bodyParts.push('');
                bodyParts.push(`> :mag: Showing 10 of ${totalErrors} errors. See workflow logs for complete list.`);
              }
              bodyParts.push('');
            }

            // Summary section (like original image)
            bodyParts.push('### :clipboard: Summary');
            bodyParts.push('');
            bodyParts.push('| Metric | Value |');
            bodyParts.push('|--------|-------|');
            bodyParts.push(`| Status | ${overallStatus} |`);
            bodyParts.push(`| Modules | ${moduleCount} |`);
            bodyParts.push(`| Scope | \`${scope}\` |`);
            bodyParts.push(`| Time | ${duration}s |`);
            bodyParts.push('');

            // Documentation Coverage (like original image)
            bodyParts.push('### :books: Documentation Coverage');
            bodyParts.push('');
            bodyParts.push('| Metric | Coverage | Detail | Goal | Status |');
            bodyParts.push('|--------|----------|--------|------|--------|');
            bodyParts.push(`| Docstrings | **${docstringCov}%** | ${documentedMethods}/${totalMethods} | >=${threshold}% | ${icon(docstringCov, threshold)} |`);
            bodyParts.push(`| Field strings | **${stringCov}%** | ${fieldsWithString}/${totalFields} | >=90% | ${icon(stringCov, 90)} |`);
            bodyParts.push(`| Field help | **${helpCov}%** | ${fieldsWithHelp}/${helpTotal} | >=50% | ${icon(helpCov, 50)} |`);
            bodyParts.push('');

            // Issues in this PR (like original image)
            bodyParts.push('### :warning: Issues in this PR');
            bodyParts.push('');
            bodyParts.push('| Type | Count |');
            bodyParts.push('|------|-------|');
            bodyParts.push(`| Missing docstrings | ${missingDocstringPr} |`);
            bodyParts.push(`| Fields without string | ${missingStringPr} |`);
            bodyParts.push(`| Fields without help | ${missingHelpPr} |`);
            bodyParts.push('');

            // Code Quality (like original image)
            bodyParts.push('### :mag: Code Quality');
            bodyParts.push('');
            bodyParts.push('| Tool | PR | Repo | Status |');
            bodyParts.push('|------|-----|------|--------|');
            bodyParts.push(`| Ruff | ${ruffChanged} | ${ruffFull} | ${icon(ruffFull, 0, true)} |`);
            bodyParts.push(`| Pylint | ${pylintChanged} | ${pylintFull} | ${icon(pylintFull, 0, true)} |`);
            bodyParts.push('');

            // Footer
            bodyParts.push('---');
            if (hasFailed) {
              bodyParts.push(':bulb: **Tip:** Fix the blocking errors above and push again.');
              bodyParts.push('');
            }
            bodyParts.push(':robot: *solt-pre-commit*');

            const body = bodyParts.join('\n');

            console.log(`Posting comment to PR #${prNumber}`);

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && 
              (c.body.includes('Validation Failed') || c.body.includes('Validation Passed') || c.body.includes('Validation Report'))
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
              console.log('Updated existing comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: body
              });
              console.log('Created new comment');
            }

      # CRITICAL: This step determines the final job status
      - name: Final validation check
        id: final-check
        if: always()
        run: |
          EXIT_CODE="${{ steps.validate.outputs.exit_code }}"
          EXIT_CODE=$(echo "$EXIT_CODE" | grep -oE '^[0-9]+' || echo 0)
          if [ -z "$EXIT_CODE" ]; then EXIT_CODE=0; fi

          FAIL_ON_WARNINGS="${{ inputs.fail-on-warnings }}"
          WARNINGS="${{ steps.summary.outputs.warnings }}"
          WARNINGS=$(echo "$WARNINGS" | grep -oE '^[0-9]+' || echo 0)

          echo "::notice::Final check - Exit code: $EXIT_CODE, Warnings: $WARNINGS, Fail on warnings: $FAIL_ON_WARNINGS"

          # Determine final result
          if [ "$EXIT_CODE" -ne 0 ]; then
            echo "result=failure" >> $GITHUB_OUTPUT
            echo "::error::Validation FAILED with blocking issues (exit code: $EXIT_CODE)"
            exit 1
          elif [ "$FAIL_ON_WARNINGS" = "true" ] && [ "$WARNINGS" -gt 0 ]; then
            echo "result=failure" >> $GITHUB_OUTPUT
            echo "::error::Validation FAILED due to warnings (fail-on-warnings enabled)"
            exit 1
          else
            echo "result=success" >> $GITHUB_OUTPUT
            echo "::notice::Validation PASSED"
            exit 0
          fi

  # ---------------------------------------------------------------------------
  # UPDATE BADGES (on push to protected branches)
  # ---------------------------------------------------------------------------
  update-badges:
    name: Update Documentation Badges
    runs-on: ubuntu-latest
    needs: validate
    if: |
      github.event_name == 'push' &&
      (github.ref == 'refs/heads/main' ||
       github.ref == 'refs/heads/master' ||
       github.ref == 'refs/heads/develop' ||
       startsWith(github.ref, 'refs/heads/17.0') ||
       startsWith(github.ref, 'refs/heads/18.0'))

    steps:
      - name: Update docstrings badge
        if: inputs.gist-id != ''
        uses: schneegans/dynamic-badges-action@v1.7.0
        continue-on-error: true
        with:
          auth: ${{ secrets.GIST_SECRET }}
          gistID: ${{ inputs.gist-id }}
          filename: ${{ inputs.badge-filename-prefix }}-docstrings.json
          label: docstrings
          message: "${{ needs.validate.outputs.docstring_cov }}%"
          valColorRange: ${{ needs.validate.outputs.docstring_cov }}
          minColorRange: 0
          maxColorRange: 100
          namedLogo: python
          logoColor: white

      - name: Update docs status badge
        if: inputs.gist-id != ''
        uses: schneegans/dynamic-badges-action@v1.7.0
        continue-on-error: true
        with:
          auth: ${{ secrets.GIST_SECRET }}
          gistID: ${{ inputs.gist-id }}
          filename: ${{ inputs.badge-filename-prefix }}-docs-status.json
          label: Documentation Standards
          message: ${{ needs.validate.outputs.docs_status }}
          color: ${{ needs.validate.outputs.docs_status == 'passing' && 'brightgreen' || 'red' }}
          namedLogo: github
          logoColor: white

  # ---------------------------------------------------------------------------
  # PRE-COMMIT HOOKS
  # ---------------------------------------------------------------------------
  pre-commit:
    name: Pre-commit Hooks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install pre-commit
        run: pip install pre-commit

      - name: Get changed files
        id: changed
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            git fetch origin ${{ github.base_ref }} --depth=1
            CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | tr '\n' ' ')
          else
            CHANGED_FILES=$(git diff --name-only HEAD~1...HEAD 2>/dev/null | tr '\n' ' ')
          fi

          VALID_FILES=""
          for f in $CHANGED_FILES; do
            if [ -f "$f" ]; then
              VALID_FILES="$VALID_FILES $f"
            fi
          done
          VALID_FILES=$(echo "$VALID_FILES" | xargs)

          FILE_COUNT=$(echo "$VALID_FILES" | wc -w | tr -d ' ')
          if [ -z "$FILE_COUNT" ]; then FILE_COUNT=0; fi

          echo "files=$VALID_FILES" >> $GITHUB_OUTPUT
          echo "count=$FILE_COUNT" >> $GITHUB_OUTPUT

      - name: Run pre-commit
        id: precommit
        run: |
          FILES="${{ steps.changed.outputs.files }}"
          COUNT="${{ steps.changed.outputs.count }}"
          if [ -z "$COUNT" ]; then COUNT=0; fi

          if [ -z "$FILES" ] || [ "$COUNT" = "0" ]; then
            echo "No files to check"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Checking $COUNT files..."
          if pre-commit run --files $FILES 2>&1 | tee precommit_output.txt; then
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Pre-commit Hooks" >> $GITHUB_STEP_SUMMARY
          echo "Files checked: ${{ steps.changed.outputs.count }}" >> $GITHUB_STEP_SUMMARY
          STATUS="${{ steps.precommit.outputs.status }}"
          if [ "$STATUS" = "passed" ]; then
            echo "All hooks passed" >> $GITHUB_STEP_SUMMARY
          elif [ "$STATUS" = "skipped" ]; then
            echo "No files to check" >> $GITHUB_STEP_SUMMARY
          else
            echo "Some hooks failed" >> $GITHUB_STEP_SUMMARY
          fi

  # ---------------------------------------------------------------------------
  # COVERAGE ANALYSIS (Optional)
  # ---------------------------------------------------------------------------
  coverage:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    if: inputs.run-coverage
    needs: validate
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}

      - name: Run coverage
        id: coverage
        continue-on-error: true
        run: |
          pip install coverage pytest pytest-cov
          mkdir -p reports
          COVERAGE=0
          if [ -d "tests" ]; then
            coverage run -m pytest tests/ -v --tb=short 2>&1 | tee reports/test_output.txt || true
            coverage report -m | tee reports/coverage.txt || true
            COVERAGE=$(tail -1 reports/coverage.txt 2>/dev/null | awk '{print $NF}' | grep -oE '[0-9]+' | head -1 || echo 0)
          fi
          if [ -z "$COVERAGE" ]; then COVERAGE=0; fi
          echo "percentage=$COVERAGE" >> $GITHUB_OUTPUT

      - name: Check threshold
        run: |
          COV="${{ steps.coverage.outputs.percentage }}"
          THRESHOLD="${{ inputs.coverage-threshold }}"
          COV=$(echo "$COV" | grep -oE '[0-9]+' | head -1 || echo 0)
          THRESHOLD=$(echo "$THRESHOLD" | grep -oE '[0-9]+' | head -1 || echo 60)
          if [ -z "$COV" ]; then COV=0; fi
          if [ -z "$THRESHOLD" ]; then THRESHOLD=60; fi
          echo "## Coverage: ${COV}%" >> $GITHUB_STEP_SUMMARY
          if [ "$COV" -ne 0 ] && [ "$COV" -lt "$THRESHOLD" ]; then
            echo "::error::Coverage ${COV}% < ${THRESHOLD}%"
            exit 1
          fi
